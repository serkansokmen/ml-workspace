{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Variational Autoencoder.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/serkansokmen/ml-workspace/blob/master/Variational_Autoencoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PWjK0ZG4UrNG",
        "colab_type": "text"
      },
      "source": [
        "# VAE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OEwpakcxUqCr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "from keras.layers import Lambda, Input, Dense, Flatten\n",
        "from keras.models import Model\n",
        "from keras.datasets import mnist\n",
        "from keras.losses import mse, binary_crossentropy\n",
        "from keras.utils import plot_model\n",
        "from keras import backend as K\n",
        "from keras.preprocessing.image import ImageDataGenerator, img_to_array\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "import pathlib\n",
        "import random\n",
        "import IPython.display as display\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import argparse\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import glob\n",
        "import cv2\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/My Drive/Google Colab"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "01hY_9Rn6Xto",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# reparameterization trick\n",
        "# instead of sampling from Q(z|X), sample epsilon = N(0,I)\n",
        "# z = z_mean + sqrt(var) * epsilon\n",
        "def sampling(args):\n",
        "    \"\"\"Reparameterization trick by sampling from an isotropic unit Gaussian.\n",
        "    # Arguments\n",
        "        args (tensor): mean and log of variance of Q(z|X)\n",
        "    # Returns\n",
        "        z (tensor): sampled latent vector\n",
        "    \"\"\"\n",
        "\n",
        "    z_mean, z_log_var = args\n",
        "    batch = K.shape(z_mean)[0]\n",
        "    dim = K.int_shape(z_mean)[1]\n",
        "    # by default, random_normal has mean = 0 and std = 1.0\n",
        "    epsilon = K.random_normal(shape=(batch, dim))\n",
        "    return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
        "\n",
        "\n",
        "def plot_results(models,\n",
        "                 data,\n",
        "                 batch_size=128,\n",
        "                 model_name=\"vae_mnist\"):\n",
        "    \"\"\"Plots labels and MNIST digits as a function of the 2D latent vector\n",
        "    # Arguments\n",
        "        models (tuple): encoder and decoder models\n",
        "        data (tuple): test data and label\n",
        "        batch_size (int): prediction batch size\n",
        "        model_name (string): which model is using this function\n",
        "    \"\"\"\n",
        "\n",
        "    encoder, decoder = models\n",
        "    x_test, y_test = data\n",
        "    os.makedirs(model_name, exist_ok=True)\n",
        "\n",
        "    filename = os.path.join(model_name, \"vae_mean.png\")\n",
        "    # display a 2D plot of the digit classes in the latent space\n",
        "    z_mean, _, _ = encoder.predict(x_test,\n",
        "                                   batch_size=batch_size)\n",
        "    plt.figure(figsize=(12, 10))\n",
        "    plt.scatter(z_mean[:, 0], z_mean[:, 1], c=y_test)\n",
        "    plt.colorbar()\n",
        "    plt.xlabel(\"z[0]\")\n",
        "    plt.ylabel(\"z[1]\")\n",
        "    plt.savefig(filename)\n",
        "    plt.show()\n",
        "\n",
        "    filename = os.path.join(model_name, \"digits_over_latent.png\")\n",
        "    # display a 30x30 2D manifold of digits\n",
        "    n = 30\n",
        "    digit_size = 28\n",
        "    figure = np.zeros((digit_size * n, digit_size * n))\n",
        "    # linearly spaced coordinates corresponding to the 2D plot\n",
        "    # of digit classes in the latent space\n",
        "    grid_x = np.linspace(-4, 4, n)\n",
        "    grid_y = np.linspace(-4, 4, n)[::-1]\n",
        "\n",
        "    for i, yi in enumerate(grid_y):\n",
        "        for j, xi in enumerate(grid_x):\n",
        "            z_sample = np.array([[xi, yi]])\n",
        "            x_decoded = decoder.predict(z_sample)\n",
        "            digit = x_decoded[0].reshape(digit_size, digit_size)\n",
        "            figure[i * digit_size: (i + 1) * digit_size,\n",
        "                   j * digit_size: (j + 1) * digit_size] = digit\n",
        "\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    start_range = digit_size // 2\n",
        "    end_range = n * digit_size + start_range + 1\n",
        "    pixel_range = np.arange(start_range, end_range, digit_size)\n",
        "    sample_range_x = np.round(grid_x, 1)\n",
        "    sample_range_y = np.round(grid_y, 1)\n",
        "    plt.xticks(pixel_range, sample_range_x)\n",
        "    plt.yticks(pixel_range, sample_range_y)\n",
        "    plt.xlabel(\"z[0]\")\n",
        "    plt.ylabel(\"z[1]\")\n",
        "    plt.imshow(figure, cmap='Greys_r')\n",
        "    plt.savefig(filename)\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJ9KCQWjlm-P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument(\"-w\", \"--weights\", help=\"Load h5 model trained weights\")\n",
        "parser.add_argument(\"-m\", \"--mse\", action=\"store_true\", help=\"Use mse loss instead of binary cross entropy (default)\")\n",
        "\n",
        "args = parser.parse_args([])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UElUp8x2jkNI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# MNIST dataset\n",
        "# (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "# print(x_train)\n",
        "\n",
        "data_path = 'data/bakarlar/original'\n",
        "\n",
        "# network parameters\n",
        "batch_size = 4\n",
        "latent_dim = 2\n",
        "epochs = 100\n",
        "\n",
        "# train_datagen = ImageDataGenerator(\n",
        "#         rescale=1./255,\n",
        "#         shear_range=0.2,\n",
        "#         zoom_range=0.2,\n",
        "#         horizontal_flip=True)\n",
        "\n",
        "# train_generator = train_datagen.flow_from_directory(\n",
        "#         data_path,\n",
        "#         target_size=(256, 256),\n",
        "#         batch_size=batch_size,\n",
        "#         class_mode=None)\n",
        "\n",
        "original_dim = train_generator.image_shape[0]\n",
        "input_shape = (original_dim, )\n",
        "intermediate_dim = 512\n",
        "\n",
        "i = 0\n",
        "train_images = []\n",
        "for filename in glob.glob(os.path.join(data_path, 'train/*.*')):\n",
        "  im = cv2.imread(filename)\n",
        "#   im = im.reshape(im.shape[0], 28, 28, 1).astype('float32')\n",
        "  print(im.shape)\n",
        "  train_images.append(im)\n",
        "#   for img in batch:\n",
        "#     img = img_to_array(img)\n",
        "#     print(img.shape)\n",
        "#     img = img.reshape(img.shape[0], 28, 28, 1).astype('float32')\n",
        "#     train_images.append(img)\n",
        "  i += 1\n",
        "  if i > 5:\n",
        "    break\n",
        "plt.imshow(train_images[0])\n",
        "\n",
        "# data_root = pathlib.Path(data_path)\n",
        "# for item in data_root.iterdir():\n",
        "#   print(item)\n",
        "\n",
        "# all_image_paths = list(data_root.glob('*.*'))\n",
        "# all_image_paths = [str(path) for path in all_image_paths]\n",
        "# random.shuffle(all_image_paths)\n",
        "\n",
        "# for img_path in all_image_paths:\n",
        "#   print(img_path)\n",
        "#   img_raw = tf.read_file(os.path.join(img_path))\n",
        "#   img_tensor = tf.image.decode_image(img_raw)\n",
        "\n",
        "#   print(img_tensor.shape)\n",
        "#   print(img_tensor.dtype)\n",
        "\n",
        "# Load custom data\n",
        "# x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.33)\n",
        "\n",
        "# image_size = x_train.shape[1]\n",
        "# original_dim = image_size * image_size\n",
        "# x_train = np.reshape(x_train, [-1, original_dim])\n",
        "# x_test = np.reshape(x_test, [-1, original_dim])\n",
        "# x_train = x_train.astype('float32') / 255\n",
        "# x_test = x_test.astype('float32') / 255\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RfYqR-HHljE3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# VAE model = encoder + decoder\n",
        "# build encoder model\n",
        "inputs = Input(batch_shape=(batch_size, original_dim), name='encoder_input')\n",
        "x = Dense(intermediate_dim, activation='relu')(inputs)\n",
        "z_mean = Dense(latent_dim, name='z_mean')(x)\n",
        "z_log_var = Dense(latent_dim, name='z_log_var')(x)\n",
        "\n",
        "# use reparameterization trick to push the sampling out as input\n",
        "# note that \"output_shape\" isn't necessary with the TensorFlow backend\n",
        "z = Lambda(sampling, output_shape=(latent_dim,), name='z')([z_mean, z_log_var])\n",
        "\n",
        "# instantiate encoder model\n",
        "encoder = Model(inputs, [z_mean, z_log_var, z], name='encoder')\n",
        "encoder.summary()\n",
        "plot_model(encoder, to_file='vae_mlp_encoder.png', show_shapes=True)\n",
        "\n",
        "# build decoder model\n",
        "latent_inputs = Input(shape=(latent_dim,), name='z_sampling')\n",
        "x = Dense(intermediate_dim, activation='relu')(latent_inputs)\n",
        "outputs = Dense(original_dim, activation='sigmoid')(x)\n",
        "\n",
        "# instantiate decoder model\n",
        "decoder = Model(latent_inputs, outputs, name='decoder')\n",
        "decoder.summary()\n",
        "plot_model(decoder, to_file='vae_mlp_decoder.png', show_shapes=True)\n",
        "\n",
        "# instantiate VAE model\n",
        "outputs = decoder(encoder(inputs)[2])\n",
        "vae = Model(inputs, outputs, name='vae_mlp')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ms1rN22OubEY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorboardcolab import TensorBoardColab, TensorBoardColabCallback\n",
        "\n",
        "tbc = TensorBoardColab()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "89Pn3xd8l4ha",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "models = (encoder, decoder)\n",
        "# data = (x_test, y_test)\n",
        "\n",
        "# VAE loss = mse_loss or xent_loss + kl_loss\n",
        "if args.mse:\n",
        "  reconstruction_loss = mse(inputs, outputs)\n",
        "else:\n",
        "  reconstruction_loss = binary_crossentropy(inputs, outputs)\n",
        "\n",
        "reconstruction_loss *= original_dim\n",
        "kl_loss = 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var)\n",
        "kl_loss = K.sum(kl_loss, axis=-1)\n",
        "kl_loss *= -0.5\n",
        "vae_loss = K.mean(reconstruction_loss + kl_loss)\n",
        "vae.add_loss(vae_loss)\n",
        "vae.compile(optimizer='adam')\n",
        "vae.summary()\n",
        "plot_model(vae,\n",
        "           to_file='vae_mlp.png',\n",
        "           show_shapes=True)\n",
        "\n",
        "if args.weights:\n",
        "  vae.load_weights(args.weights)\n",
        "else:\n",
        "  # train the autoencoder\n",
        "  vae.fit(x_train,\n",
        "          epochs=epochs,\n",
        "          batch_size=batch_size,\n",
        "          validation_data=(x_test, None),\n",
        "          callbacks=[TensorBoardColabCallback(tbc),])\n",
        "#   vae.fit_generator(train_generator,\n",
        "#           epochs=epochs,\n",
        "#           steps_per_epoch=4,\n",
        "#           callbacks=[TensorBoardColabCallback(tbc),])\n",
        "  vae.save_weights('models/vae_mlp_mnist.h5')\n",
        "\n",
        "plot_results(models,\n",
        "             data,\n",
        "             batch_size=batch_size,\n",
        "             model_name=\"vae_mlp\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eom85VpWuXCZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}